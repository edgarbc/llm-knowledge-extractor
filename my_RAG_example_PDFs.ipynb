{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1I2D1aKzNnvHB4fITgbF2FSWYQrB4UHHA",
      "authorship_tag": "ABX9TyPdWnKC1ViaQsUiwsqwcR5B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edgarbc/llm-knowledge-extractor/blob/main/my_RAG_example_PDFs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of a simple RAG to query an LLM on local data from PDFs.\n",
        "\n",
        "Edgar Bermudez\n",
        "\n",
        "November, 2023"
      ],
      "metadata": {
        "id": "4AN1AJ_Ldnbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval augmented generation (RAG) provides a way to optimize the output of an LLM with targeted information without modifying the underlying model itself.\n",
        "\n",
        "This targeted information can be more up-to-date than the LLM and can be specific to a particular organization and industry [oracle.com](https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/).\n",
        "\n",
        "The RAG method works by fetching up-to-date or context-specific data from an external database and making it available to an LLM when asking it to generate a response. This approach can help to reduce the likelihood of hallucinations  and improve the accuracy of the LLMs with more up-to-date information.\n",
        "\n",
        "The key points are :\n",
        "\n",
        "- how to ingest data so that can be passed to the encoder part of the LLM to product the embeddings\n",
        "- map the query to the space with the new embeddings using the same encoder and use the decoder part of the model to generate the response"
      ],
      "metadata": {
        "id": "CgjshQRXha67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install llama-index\n",
        "!pip install tiktoken\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc7fn6_FYGRj",
        "outputId": "b2cfa3f1-2885-4b6b-a594-d77553ead618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.9.0.post1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.0.23)\n",
            "Requirement already satisfied: aiostream<0.6.0,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.5.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.12.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.5.14)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.2.14)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.25.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.23.5)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.5.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.0)\n",
            "Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.26.18)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->llama-index) (3.20.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index) (1.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (4.66.1)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index) (1.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index) (1.10.13)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index) (1.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.3.3->llama-index) (2.31.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai>=1.1.0->llama-index) (1.1.3)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->llama-index) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken>=0.3.3->llama-index) (3.3.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx->llama-index) (0.14.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.17.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzOU-GB3XsLc"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import tiktoken\n",
        "from llama_index import ServiceContext, LLMPredictor, OpenAIEmbedding, PromptHelper\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.text_splitter import TokenTextSplitter\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index import set_global_service_context"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the API_KEY to be able to use gpt-3.5-turbo LLM from OpenAI.\n",
        "\n",
        "TODO: extend to an opensource LLM from HuggingFace."
      ],
      "metadata": {
        "id": "B_Pl_Jn9eRjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "zc-eshEHYLCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY']=\"API_KEY\""
      ],
      "metadata": {
        "id": "LtZseFF3Y4Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the PDF documents stored in the local directory data_dir. Here we assume that PDFs are uploaded into the data_dir. However, if you want to keep your data in the cloud in a permanent storage like google cloud storage (GCS) you can use unstructured (https://pypi.org/project/unstructured/)."
      ],
      "metadata": {
        "id": "AgsOg_qkeBf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/paper_data/\"\n",
        "!ls \"/content/drive/MyDrive/Colab Notebooks/paper_data/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm0MzkyYHCR6",
        "outputId": "3b904761-5712-41fb-e54d-7940615f22c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2209.07162.pdf\t2302.05543.pdf\t2307.15208.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from llama_index.readers.file.base import SimpleDirectoryReader\n",
        "documents = SimpleDirectoryReader(input_dir=\"/content/drive/MyDrive/Colab Notebooks/paper_data/\").load_data()"
      ],
      "metadata": {
        "id": "VEc18SgYY-vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that loading the PDFs was successful"
      ],
      "metadata": {
        "id": "2qmz_PT8JDA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbxhvYshH2ak",
        "outputId": "33ee2d28-5915-4cb3-8eb8-9b9b41e255cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id_='f4b06318-3b9d-4787-83e3-7259220b0055', embedding=None, metadata={'page_label': '2', 'file_name': '2209.07162.pdf', 'file_path': '/content/drive/MyDrive/Colab Notebooks/paper_data/2209.07162.pdf', 'file_type': 'application/pdf', 'file_size': 6234415, 'creation_date': '2023-11-07', 'last_modified_date': '2023-11-07', 'last_accessed_date': '2023-11-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='f1935e7d9d77101e716dec79166ad2d24671a696ce9902cfa006b83dcc4a8a6e', text='2 Pinaya et al.\\nsentences [8,26]. During the same period, medical image analysis also made re-\\nmarkable breakthroughs by applying deep neural networks to solve tasks such\\nas segmentation, structure detection, and computer-aided diagnosis (detailed re-\\nview available at [20,27]). However, one current limitation of medical imaging\\nprojects is the lack of availability of large datasets. Medical data are costly and\\nlaborious to collect, and privacy concerns create challenges to data sharing by\\nrestricting publicly available medical datasets to up to a few thousand examples.\\nThis limitation creates a bottleneck on models’ generalizability and hampers the\\nrate at which cutting-edge methods are deployed in the clinical routine.\\nGenerating synthetic data with privacy guarantees provides a promising al-\\nternative, allowing meaningful research to be carried out at scale [15,14,33].\\nTogether with traditional data augmentation techniques (e.g., geometric trans-\\nformations), these synthetic data could complement real data to dramatically\\nincrease the training set of machine learning models. Generative models learn\\nthe probability density function underlying the data they are trained on and\\ncan create realistic representations of examples which are diﬀerent from the\\nones present in the training data by sampling from the learned distribution.\\nHowever, generating meaningful synthetic data is not easy, especially when con-\\nsidering complex organs like the brain.\\nNowadays, Generative Adversarial Networks (GANs) have been applied in\\nvariousﬁeldstocreatesyntheticimages,producingrealisticandclearimagesand\\nachieving impressive performance [7,32]. In the medical ﬁeld, for example, [17]\\ncombine variational autoencoders with GANs to generate various modalities of\\nwhole brain volumes from a small training set and achieved a better performance\\ncompared to several baselines. However, since their study resized the images to a\\nsmall volume before training, with a size of 64 ×64×64 voxels, their synthetic\\nmedical images did not replicate many essential ﬁner details. In addition, due to\\nthe prevalence of 3D high-resolution data in the ﬁeld, researchers tend to have\\ntheir models restrained by the amount of GPU memory available. To mitigate\\nthis problem, [31] proposed a 3D GAN with a hierarchical structure which is able\\nto generate a low-resolution version of the image and anchor the generation of\\nhigh-resolution sub-volumes on it. With this approach, the authors were able to\\ngenerate impressive realistic 3D thorax CT and brain MRI with resolutions up\\nto 256 ×256×256 voxels. Despite generating great interest, GANs still come\\nwith inherent challenges, such as being notoriously unstable during training and\\nfailing to converge or to capture the variability of the generated data due to\\nmode collapse issues [16].\\nRecently, diﬀusion models caught the attention of the machine learning com-\\nmunity by showing promising results when synthesizing natural images. They\\nhave rivalled GANs in sample quality [9] while building upon a solid theoretical\\nfoundation. Not only have they reported impressive photorealism unconditioned\\nimages, but they have also been used to create images conditioned in classes and\\ntext sentences (using techniques like classiﬁer-free guidance [13]), with excep-\\ntional results on models like Latent Diﬀusion Models [23], DALLE 2 [22], and\\nImagen [25].', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to split the text into tokens. We can use tiktoken for this and openAI gpt to produce the encodings."
      ],
      "metadata": {
        "id": "4YXW-LEZJZWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L2f3wwuYZMmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028fce09-3277-47ec-8310-02c82bcc5680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you can parse the documents into nodes using the default values\n",
        "text_splitter = TokenTextSplitter(\n",
        "    separator=\" \",\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=20,\n",
        "    backup_separators=[\"\\n\"],\n",
        "    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
        ")\n",
        "\n",
        "node_parser = SimpleNodeParser.from_defaults()"
      ],
      "metadata": {
        "id": "eZkRECqYZqk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or you can be more specific to define how to parse the documents. For example:"
      ],
      "metadata": {
        "id": "lunG3FZ3NxKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import SentenceSplitter\n",
        "text_splitter=SentenceSplitter(\n",
        "    separator=\" \",\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=20,\n",
        "    paragraph_separator=\"\\n\\n\\n\",\n",
        "    secondary_chunking_regex=\"[^,.; ]+[,.; ]?\",\n",
        "    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "JZD-bY14aAMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=256)\n",
        "embed_model = OpenAIEmbedding()\n",
        "prompt_helper = PromptHelper(\n",
        "    context_window=4096,\n",
        "    num_output=256,\n",
        "    chunk_overlap_ratio=0.1,\n",
        "    chunk_size_limit=None\n",
        ")\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=llm,\n",
        "    embed_model=embed_model,\n",
        "    node_parser=node_parser,\n",
        "    prompt_helper=prompt_helper\n",
        ")"
      ],
      "metadata": {
        "id": "a0HJ5_LGbKQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.vector_store.base import VectorStoreIndex\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "id": "vaYLju0ScARL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now define how we are going to query the knowledge (vector) database"
      ],
      "metadata": {
        "id": "dc4sJqoOOH3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine=index.as_query_engine(service_context=service_context)\n"
      ],
      "metadata": {
        "id": "QLF18bF9cTXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's ask something specific that is present in the documents we augmented it with:"
      ],
      "metadata": {
        "id": "TXwFod-hOQRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"Who proposed a 3D GAN with a hierarchical structure?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "NUVpMNgacrcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "377ab0e1-6a63-4644-8e86-33546fa707ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun et al. proposed a 3D GAN with a hierarchical structure.\n"
          ]
        }
      ]
    }
  ]
}